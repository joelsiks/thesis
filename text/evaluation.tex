
When evaluating memory allocators, two primary metrics stand out: performance and memory usage, specifically fragmentation. Performance entails measuring the time required to perform allocation and/or free request, while fragmentation assesses how efficiently the allocator utilizes the memory. We will also look at something that is interesting to look at in regard to the goals of this thesis, a metric that we call ``hole fillability'', which describes how well an allocator can fill holes in a memory heap.

\subsection{Allocator Configurations}

% What versions of the allocator am I testing and how can the reader repeat:
% - Reference allocator
% - General and optimized allocators (describe with configuration variables)

% The goal is to measure relative performance between the different versions.

The three different versions of the TLSF allocator developed in this work that will be evaluated and compared in this section are: the reference TLSF implementation, the general version and the optimized version for ZGC. 

Given that the optimized allocator is adapted specifically for ZGC, its use-cases are such that it is not straightforward to compare it against the other versions. To get around this, we will look at test-cases where the use-case for the optimized version overlap with the other two allocators. 

Testing this way is indeed not comprehensive and exhaustive. However, reiterating the goals for this thesis, which is to find out whether it is reasonable to adapt an allocator for use in ZGC and what challenges it presents, limiting the scope of the testing is also reasonable. Further and more comprehensive testing and evaluation will not be considered in this work, partly due to time constraints, but mainly due to it not being the primary focus of this thesis. The goal of evaluating the work in this thesis is to give some metric at which to reason about the work done here and is meant as a guide to developing this work further.

The general and optimized versions are described by their configuration variables from the base implementation, as shown in Table~\ref{table:configuration-variables}.

\begin{table}[H]
\centering
\begin{tabular}{lllll}
\hline
Configuration Variable  & General  & \multicolumn{3}{l}{Optimized} \\ \hline
First-Level Index       & 32       & \multicolumn{3}{l}{14}        \\
Second-Level Index      & 5        & \multicolumn{3}{l}{2}         \\
Minimum Block Size      & 32       & \multicolumn{3}{l}{16 }       \\
Use Second Levels       & True     & \multicolumn{3}{l}{False}     \\
Use Deferred Coalescing & False    & \multicolumn{3}{l}{True}      \\
Block Header Length     & 32       & \multicolumn{3}{l}{0}        
\end{tabular}
\caption{Configuration variables for the general and optimized versions of the allocator.}
\label{table:configuration-variables}
\end{table}

\subsection{Measuring Performance}

% Explain why we are not using standardized benchmarks (becuase the use-case is narrowly scoped towards GC)
Ideally we would want to use the allocators in a standardized benchmark suite, like DaCapo or SPECjbb, that base themselves on realistic workloads. However, again, as the use-case of the optimized version is narrowed down significantly, it does not overlap with the tests in neither of the benchmark suites, or any other that we know about. Instead, we will test the allocators on a set of tests we create ourselves, that will give an impression of the performance of the allocator. To make the tests more credible and based on realistic workloads, we will extract allocation and free patterns from the real-world Linux utility programs: X, Y and Z. Extracting and using allocation and free patterns this way we feel give a fairer workload rather than generating a synthetic pattern, which gives the testing data at least some credibility.

The first test that we will measure is a single allocation and free. This will give an impression of how the allocators compare in their most basic functionality. The second will be a comprehensive test using the extracting allocation patterns. To get as accurate results as possible from these tests, we will run them multiple times and report an average as well as worst result.

% What test cases will we consider:
% - Single allocation/free (wall clock time)
% - A distribution of allocations (collected from dacapo)
%   - A mix of allocations/frees, with syntentically added frees (since we cannot gather free call data from dacapo... must mention why)
%   - This is as close to integrating to OpenJDK we can get without actually integrating.
%   - "Vi kan lätt återskapa en allokeringshistorik men inte free-historik", möjlig felkälla.
%
% We have focus on these tests because...

% Describe how we've extracted allocation/free patterns.

% How much memory does the allocator have in its pool for the tests.
% The pool is statically allocated and does not change in size during runtime.
%  - And why are we doing this...

\subsection{Measuring Fragmentation}

To measure internal fragmentation we will use a set of counter that are incremented and decremented during allocations and frees respectively. For each call to allocate, the number of bytes the user has requested along with the actual amount of memory used by the heap is recorded. Conversely, when a user frees a piece of memory, the corresponding amount of requested bytes and actual memory used are subtracted from their respective counters. With the two counters at hand, the internal fragmentation can be calculated as a percentage of the memory that is used as follows:
\[
\text{internal\_fragmentation} = \frac{\text{total requested bytes}}{\text{total allocated bytes}}
\]
\subsection{Measuring Hole Fillability}

Measuring internal fragmentation is rather straightforward and well-defined. External fragmentation is however more tricky to define in a single metric that can easily be computed and compared. Instead, to gauge external fragmentation, and also to measure directly what this thesis aims to solve, we will measure a metric we call hole fillability.

Hole fillability is how well an allocator can fill available holes in a given state of the underlying memory of allocated and free blocks. This is similar to external fragmentation in the sense that hole fillability will tell us what the largest allocation that can be made is and how much memory is available to us. This will depend greatly on the size of the block headers, how much padding allocations must have and coalescing behavior, which differs between the different versions of the allocator.

To do this, we will gather snapshots of the heap when using the allocator with the allocation and free patterns collected for measuring performance. The snapshots will be randomly selected and each version of the allocator will be compared on the same snapshot to see how well they are able to fit new data inside holes in the heap.

% Measure fragmentation:
% - Internal fragmentation (through counters during allocations/frees)

% - We will skip measuring external fragmentation, becuase it is hard to define because...

%Note that while integration with ZGC would provide a definitive validation of the allocator's functionality, such integration testing is excluded from the scope of this thesis, as mentioned in Section~\ref{sec:delimitations}. With this said, investigating and implementing adaptations into an allocator in itself will contribute not only to future integration, but adapting any kind of free-list based allocator for use in garbage collection or similar areas.

\subsection{Machines to Collect Data}

Performance benchmarks have been run on a single native machine. The specifications for this machine is shown in Table~\ref{table:machine}. Each version of the allocator is compiled using GCC 11.4.0 with the C++14 (\texttt{-std=c++14}) standard using optimization level three (\texttt{-O3}).

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{p{1cm}p{2in}p{20mm}p{1cm}p{50pt}p{2cm}p{1cm}p{1cm}p{2cm}p{15mm}}
Type   & \multicolumn{3}{p{1cm}}{OS}                 & Linux Kernel Version      & CPU                                      & Threads Per Core & Cores Per Socket & CPU Attached To Number Of Stockets & Memory \\
\hline
Native & \multicolumn{3}{p{1cm}}{Ubuntu 22.04.4 LTS} & Kernel 5.15.0-101-generic & AMD Opteron(TM) Processor 6274 @ 2.2 GHz & 2                & 8                & 2                                  & 110GB 
\end{tabular}
\caption{Machine used to run benchmarks.}
\label{table:machine}
\end{table}

% What hardware are we collecting data on:
% - OS, CPU (threads, cores), memory
% - Why we have chosen this machine(s)

% L1d:                   512 KiB (32 instances)
% L1i:                   1 MiB (16 instances)
% L2:                    32 MiB (16 instances)
% L3:                    24 MiB (4 instances)

% How are we compiling the allocator
% - g++, optimization flags, c++ version
