
When evaluating memory allocators, two primary metrics stand out: performance and memory usage, specifically fragmentation. Performance entails measuring the duration it takes to perform certain operations, while fragmentation assesses how efficiently the allocator utilizes the available memory.

\subsection{Evaluation Delimitations}

To reiterate, we will not integrate the allocator into ZGC, as there is simply not enough time to do so in the context of this work. Integration into ZGC would provide a natural environment to test and evaluate the allocator in. Choosing not to do this impacts the way in which we are able to evaluate the allocators definitively.

The optimized version of the allocator is tailored toward ZGC to the extent that it limits the pattern of allocations and frees that can be used on it. This makes it hard to find overlapping areas where the different versions of the allocator can be fairly compared and to finding suitable patterns given the limits on allocation and heap size that the optimized version has. Part of what makes this hard is that even when applying the same pattern of allocations and frees, their respective heaps will be filled differently, and at different speeds. This is mainly due to the fact that the different versions have different sizes for the block header, making them use varying amounts of memory. For example, when the general version fills its heap, the optimized version may still have some portion of unused memory available. Additionally, the optimized version does not support immediate coalescing. This affects the way that the heap is filled and makes it unclear when to perform explicit coalescing so that the same allocation requests can be fulfilled on all versions of the allocator.

Because of these difficulties, we have chosen to limit evaluation to cases that can be fairly compared and reasoned about. This method is not exhaustive, which is often difficult when evaluating allocators in general, but will provide a level on which to reason about the performance of the versions of the allocator.

\subsection{Allocator Configurations}

The three different versions of the TLSF allocator developed in this work that will be evaluated and compared in this section are: the reference TLSF implementation, the general version and the optimized version for ZGC. 

The general and optimized versions are described by their configuration variables from the base implementation, as shown in Table~\ref{table:configuration-variables}.

\begin{table}[H]
\centering
\begin{tabular}{lllll}
\hline
Configuration Variable    & General  & \multicolumn{3}{l}{Optimized} \\ \hline
First-Level Index         & 32       & \multicolumn{3}{l}{14}        \\
Second-Level Index (log2) & 5        & \multicolumn{3}{l}{2}         \\
Minimum Block Size        & 32       & \multicolumn{3}{l}{16 }       \\
Use Second Levels         & True     & \multicolumn{3}{l}{False}     \\
Use Deferred Coalescing   & False    & \multicolumn{3}{l}{True}      \\
Block Header Length       & 32       & \multicolumn{3}{l}{0}        
\end{tabular}
\caption{Configuration variables for the general and optimized versions of the allocator.}
\label{table:configuration-variables}
\end{table}

\subsection{Measuring Performance}

For performance, we will look at two different benchmarks: single-allocation and patterns that combine both allocate and free requests.

For single allocation, we will only measure allocation of a single size, 64 bytes. This is because the design of TLSF is such that it sets an upper bound on the number of instructions required to allocate memory, which makes it unnecessary to measure allocation of multiple different sizes, as performance only depends on the availability of suitable blocks in free-lists. Three cases will be measured, of which the first two are: where there exists a block in the heap that perfectly align with the allocation size, and another where there only exists a block that is larger than the allocation size. The latter case requires, in addition to doing the same operations as in the first case, searching the bitmap for a larger block and splitting it. This case is also used as the worst case test by M. Masmano et al.~\cite{TLSF} for the evaluation of the original TLSF design. The third case will be performing the first case and also initializing the returned memory by filling it with zeros. This is to see how initializing the memory affects performance, as this is done after allocating memory in ZGC. Measuring these three cases will provide insight into both normal and worst-case allocation performance.

In a more comprehensive performance test, we will apply a pattern of both allocation and free requests. Due to the limitations mentioned in the previous section, we will use a pattern that adhere to the requirements of the optimized version. Patterns will be recorded from the GNU utilities: \texttt{cat}, \texttt{grep}, \texttt{ls}, \texttt{nano}, \texttt{sed} and \texttt{wc}, which are then applied to the different versions of the allocator to compare performance. The reason this is done instead of just using the allocators in the programs is to highlight only the performance of allocating and freeing. Using the allocators in the programs would spend significantly more time doing program logic instead of using the allocator, making it more difficult to see the performance impact of changing allocator.

In order to fairly compare the versions of the allocators when applying allocation and free patterns, we will disable coalescing for the reference and general versions. This is because the optimized version does not support immediate coalescing and only implements explicit coalescing, which is intentionally slower due to the fact it should be used only as a last-resort. Disabling immediate coalescing is also reasonable from the perspective that it is likely not desirable as often in Z, making the test more adapted to the intended use-case of the optimized version.

% To measure performance we will look at the time it takes to perform the three cases for the three versions of the allocator. The measurements will tell us how the adaptations that have been made to the optimized version affect its performance when doing its most fundamental operation. To improve the reliability of the results, measurements will be recorded by running the tests 100000 times and taking the mean value. Time is measured using the POSIX function \texttt{clock\_gettime()} with the clock set to \texttt{CLOCK\_MONOTONIC\_RAW}. Measurements are made right before and after calling \texttt{malloc()} on the allocator and the difference between the two is reported.

\subsubsection{Programs to Record Patterns From}

% Börja med att förklara vilka program jag använt (namn + version) och hur jag kört document

Table~\ref{table:pattern-programs} show the specific versions and commands of the programs used to record patterns from. All commands except \texttt{ls} have performed their respective actions on the same file called \texttt{pi.txt}. The file contains the first 10000 decimals of pi, linebroken every 100 decimal to create 100 lines of 100 decimals. The \texttt{ls} command has been run inside a directory containing only the file \texttt{pi.txt}. The \texttt{nano} command opens \texttt{pi.txt} and exits immediately.

\begin{table}[H]
\centering
\begin{tabular}{llp{10.4cm}}
\textbf{Program} & \textbf{Version} & \textbf{Command} \\ \hline
cat  & 8.32 & cat pi.txt\\ \hline
grep & 3.7  & grep 1425 pi.txt\\ \hline
ls   & 8.32 & ls \\ \hline
nano & 6.2  & nano pi.txt\\ \hline
sed  & 4.8  & sed "s/1425/5241/g" pi.txt\\ \hline
wc   & 8.32 & wc pi.txt\\ \hline
\end{tabular}
\caption{The programs used to record allocation and free patterns from and the specific commands used to execute them.}
\label{table:pattern-programs}
\end{table}

Patterns have been recorded using the \texttt{LD\_PRELOAD} environment variable in Linux to hook into \texttt{malloc()} and \texttt{free()} and output the requests, in order, to a file. The recorded pattern is then replayed on the different versions of the allocator in the same order they were recorded, essentially emulating the usage of the allocator in isolation.

\subsection{Measuring Fragmentation}

When reasoning about internal fragmentation in the different versions of the allocator, we are concerned about two things: wasted space due to block header overhead and wasted space due to padding/alignment. To gauge internal fragmentation without having to rely on a pattern of allocations and free requests, we will numerically analyze the worst-case of fragmentation for all versions of the allocator. To make the analysis suited toward ZGC, we place limitations on the allocators that align with small pages of ZGC. The heap size is set to 2MB and allowed allocation sizes are in the range [16B, 256KB]. Additionally, even though one might use different alignment for the different allocators, to fairly compare them we will align allocations to 8 bytes on all allocators, since we are running on a 64-bit system.

We will examine two worst-cases: (1) when maximum space is wasted due to both block header and padding, and (2) when maximum space is wasted only due to block header. For both cases, the heap will be filled with as many blocks as possible of the smallest possible allocation size. Keeping the allocation size range of ZGC small pages in mind, 17 bytes and 16 bytes will be allocated in the first and second case respectively, since 17 bytes requires maximum padding and 16 bytes require no padding.

Internal fragmentation is calculated in the following way, where the total allocation consists of block header, allocation and padding. num\_blocks is round down to the nearest integer since a fraction of a block cannot be created.
\begin{align*}
    \text{waste} &= \text{block\_header} + \text{padding} \\\\
    \text{num\_blocks} &= \frac{2\text{MB}}{\text{block\_header + allocation + padding}} \\\\
    \text{internal\_fragmentation} &= \frac{\text{num\_blocks} \cdot \text{waste}}{2\text{MB}}
\end{align*}

From looking at the sizes of the block headers, it is straightforward to conclude which allocator uses less memory overall since smaller header would mean less memory used overall when allocating the same size. However, the goal of this evaluation is to see how much memory could theoretically be wasted when using different versions of the allocator, which also includes padding. The final results will give us a range of worst-case memory wastage, which will vary depending on the amount of padding that is applied. In practice, the allocator will most likely never reach the calculated worst-case since all allocations will unlikely be of the same and lowest possible size. However, the results will provide an upper-bound that can be used to reason about the memory efficiency of the allocator.

% It would be more practical and descriptive to measure the average 

% It would be interesting to see what the average fragmentation would look like, however, since 

\subsection{Machine to Collect Data}

Performance benchmarks are run on the machine with the specifications shown in Table~\ref{table:machine}. Each version of the allocator is compiled using GCC 11.4.0 with the C++14 (\texttt{-std=c++14}) standard using optimization level two (\texttt{-O2}).

\begin{table}[H]
    \centering
\begin{tabular}{cp{11.3cm}}
    \textbf{Configuration} & \textbf{Machine} \\ \hline
CPU           & AMD Opteron 6274 @ 2.2 GHz with 8 cores (2 threads/core)\\ \hline
Memory        & 110GB                                                   \\ \hline
L1 Cache      & 512KB (L1d), 1MB (L1i)                                  \\ \hline
L2 Cache      & 32MB                                                    \\ \hline
L3 Cache      & 24MB                                                    \\ \hline
System        & Ubuntu 22.04.4 LTS (Jammy Jellyfish)                    \\ \hline
Kernel        & Linux 5.15.0-101-generic
\end{tabular}
\caption{Machine used to collect data.}
\label{table:machine}
\end{table}
